
<!DOCTYPE html>
<html lang="en">
<head>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>

    <link rel="stylesheet" type="text/css" href="https://michael-adcock.github.io/theme/stylesheet/style.min.css">

  <link rel="stylesheet" type="text/css" href="https://michael-adcock.github.io/theme/pygments/github.min.css">
  <link rel="stylesheet" type="text/css" href="https://michael-adcock.github.io/theme/font-awesome/css/font-awesome.min.css">


    <link href="https://michael-adcock.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Michael Adcock Atom">



  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />


<meta name="author" content="Michael Adcock" />
<meta name="description" content="An Area Classification of Consumer Vulnerability in the UK using 2011 census data" />
<meta name="keywords" content="R, k-means, principal component analysis, PCA, census, consumer vulnerability">
<meta property="og:site_name" content="Michael Adcock"/>
<meta property="og:title" content="An Area Classification of Consumer Vulnerability in the UK"/>
<meta property="og:description" content="An Area Classification of Consumer Vulnerability in the UK using 2011 census data"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="https://michael-adcock.github.io/consumer_vulnerability.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2018-05-26 20:00:00+01:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="https://michael-adcock.github.io/author/michael-adcock.html">
<meta property="article:section" content="posts"/>
<meta property="article:tag" content="R"/>
<meta property="article:tag" content="k-means"/>
<meta property="article:tag" content="principal component analysis"/>
<meta property="article:tag" content="PCA"/>
<meta property="article:tag" content="census"/>
<meta property="article:tag" content="consumer vulnerability"/>
<meta property="og:image" content="">

  <title>Michael Adcock &ndash; An Area Classification of Consumer Vulnerability in the UK</title>

</head>
<body>
  <aside>
    <div>
      <a href="https://michael-adcock.github.io">
        <img src="https://michael-adcock.github.io/theme/img/profile.png" alt="" title="">
      </a>
      <h1><a href="https://michael-adcock.github.io"></a></h1>

<p>Data Scientist</p>

      <ul class="social">
        <li><a class="sc-github" href="https://github.com/michael-adcock" target="_blank"><i class="fa fa-github"></i></a></li>
        <li><a class="sc-linkedin" href="https://www.linkedin.com/in/m-t-adcock/" target="_blank"><i class="fa fa-linkedin"></i></a></li>
      </ul>
    </div>


  </aside>
  <main>


<article class="single">
  <header>
    <h1 id="consumer_vulnerability">An Area Classification of Consumer Vulnerability in the UK</h1>
    <p>
          Posted on Sat 26 May 2018 in <a href="https://michael-adcock.github.io/category/posts.html">posts</a>


    </p>
  </header>


  <div>
    <p>Geodemograpic classifications are often used for marketing purposes, focusing on who to target with certain products. In this post we will be having a look at a classifcation I have created that takes a different approach and identifies areas where it may be unsuitable to directly target people without considering their wellfare. This classification identifies output areas of high consumer vulnerability in the United Kingdom using a k-means approach coupled with principal compent anlysis. As the focus of this blog is on the application of data science methods, I won't be going too deeply into the background consumer vulnerability in this post or disscussing the results at length. If you are intersted in finding out more there is a publication expected for this work which I will share when it is available.</p>
<p>The mapped results for this work are hosted on Consumer Center Research Center Maps here. If you haven't used this map service before then I highly recommend it. There are loads of great maps to view.</p>
<p><img alt="center" src="https://michael-adcock.github.io/figures/cdrc_leeds_map.png" /></p>
<h2>A Definition of Consumer Vulnerability</h2>
<p>Before we get into looking at the methods I should explain what we really mean by consumer vulnerabily. The formal definition used in this work is:</p>
<p><strong>Consumer vulnerability is the risk that a consumer's mental, physical or financial welfare may be damaged when engaging in a market interaction.</strong></p>
<p>As well as having the above definition we need to be able to measure consumer vulnerabilty if we are going to create a useful classification. I have use five dimensions of vulnerabilty wich I have adapted the dimensions from a Europen Commission report exploring consumer vulnerability in Europe.</p>
<ol>
<li>
<p>Risk of harm occurring to the consumer as a result of a negative transafore I highly ction outcome.</p>
</li>
<li>
<p>The ability of the consumer to maximise their own well-being.</p>
</li>
<li>
<p>The ability of the consumer to access suitable information about products or services.</p>
</li>
<li>
<p>The ability of the consumer to access suitable products and services.</p>
</li>
<li>
<p>Susceptibility of the consumer to market practices.</p>
</li>
</ol>
<p>When making use of these dimensions to measure vulnerability the following factors should be considered: individual characteristics (e.g. age, income), transient states of the consumer (e.g. illness), external conditions (e.g. prejudice) on the consumer, and market forces acting on the consumer.</p>
<p>This definition and the five dimensions of vulnerability can be considered from the perspective of different markets (e.g. energy sector, financial sector and the gambling industry) which will change the vulnerability of customers in respect to these markets. This can be controled by selecting appropiate variables.</p>
<p>Now we have deifined what consumer vulnerability is we can lookat how the classification itself was created.</p>
<h2>Data</h2>
<p>The raw data used for this study was the UK Census from 2011. The file from which this data is taken is the input data from Output Area Classification (OAC) 2011. This file includes all variables (167 in total) prior to variable selection for OAC. The geographical units used are output areas and the data is structured in the form of counts for each variable per output area (except in certain specified cases where rates are used). The dataset and lookup table was downloaded from http://geogale.github.io/2011OAC found in the "Input Data" section under the title "2011 OAC 167 Initial Variables Dataset".</p>
<h2>Variable Selection</h2>
<p>I can not stress enough how important careful varibale selection is when performing a k-means clustering (or any clustering for that matter). Don't select a varibale usless you have good justification for using it defined by your aims (however, also don't worry about redundency for now, we'll look at corrolation between variables in the next section.)</p>
<p>The variables were used create a general consumer vulnerabilty classification (rather than aimed at a specific market) and were selected against the five dimensions listed above.</p>
<p>I won't give a list of the variables that I selected here as they are all displayed in my figures later on. I have provided a summary table though to show have many varibles represent each of my dimesions.</p>
<p>I have also pooled variables where they were no more informative when considered separately, from a consumer vulnerability perspective, than they were  when considered together.  For example, for the purpose of consumer vulnerability, married people with children, are functionally the same as cohabiting people with children so these variables were pooled into a single variable. This step helped to improve final interpretability of the clusters.</p>
<h2>Data Treatment</h2>
<p>To acheive a sucessfull clustering using k-means the data will normally need to be treated. This is due to the assumptions of k-means which I will discuss as we move along.</p>
<p>All analysis for this project was done in R but could easily be carried out in Python (or another language) if you prefer. First we load the libraries we will need and read in our data.</p>
<div class="highlight"><pre><span></span><span class="kn">library</span><span class="p">(</span>plyr<span class="p">)</span>
<span class="kn">library</span><span class="p">(</span>RColorBrewer<span class="p">)</span>
<span class="kn">library</span><span class="p">(</span>tidyverse<span class="p">)</span>
<span class="kn">library</span><span class="p">(</span>rgdal<span class="p">)</span>
<span class="kn">library</span><span class="p">(</span>parallel<span class="p">)</span>
<span class="kn">library</span><span class="p">(</span>plotrix<span class="p">)</span>
<span class="kn">library</span><span class="p">(</span>ggplot2<span class="p">)</span>
<span class="kn">library</span><span class="p">(</span>reshape2<span class="p">)</span>
<span class="kn">library</span><span class="p">(</span>sf<span class="p">)</span>

<span class="c1"># Load data</span>
oa_data <span class="o">&lt;-</span> readr<span class="o">::</span>read_csv<span class="p">(</span><span class="s">&quot;2011_OAC_Raw_uVariables.csv&quot;</span><span class="p">)</span>
variable_choices <span class="o">&lt;-</span> readr<span class="o">::</span>read_csv<span class="p">(</span><span class="s">&quot;K-means_version_5_variables.csv&quot;</span><span class="p">)</span>
lookup_table <span class="o">&lt;-</span> readr<span class="o">::</span>read_csv<span class="p">(</span><span class="s">&quot;2011_OAC_Raw_uVariables_Lookup.csv&quot;</span><span class="p">)</span>
</pre></div>


<p>As an aside you may notice my habit of prefixing my functions with the library they come from. This isn't normal convention in R but I find it helps reduce to bugs when multiple libraies have been loaded with shared function names. I would also argue that this is a cleaner way of programming as someone else reading my code immediatly knows which library a function has come from.</p>
<h4>Counts to Percentages</h4>
<p>In order to use the selected data for clustering, I have converted to percentages for each output area. In order to correctly find the percentage for each variable the correct denominator needed to be identified (e.g. total population, population over 16) for the percentage calculation. I have stored the denominator for each variable to correctly calculate the percentage in the variable choice csv file. In the cases of a variable describing a rate rather than a count no conversion is neccessary.</p>
<div class="highlight"><pre><span></span><span class="c1"># Subset OA data and calculate percentages using demominators </span>
count_to_percentage <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>input_df<span class="p">,</span> variable_choices<span class="p">)</span> <span class="p">{</span>
  tibble<span class="o">::</span>as.tibble<span class="p">(</span>input_df<span class="p">[</span>variable_choices<span class="o">$</span>VariableCode<span class="p">]</span> <span class="o">/</span> 
                      input_df<span class="p">[</span>variable_choices<span class="o">$</span>StatisticalUnit<span class="p">]</span> <span class="o">*</span> <span class="m">100</span><span class="p">)</span>
<span class="p">}</span>

<span class="c1"># Prepare raw data to pecentages</span>
cv_pct_data <span class="o">&lt;-</span> oa_data <span class="o">%&gt;%</span>
  tibble<span class="o">::</span>add_column<span class="p">(</span><span class="s">&quot;Rate&quot;</span> <span class="o">=</span> <span class="m">100</span><span class="p">)</span> <span class="o">%&gt;%</span>  <span class="c1"># Add a column of just 100&#39;s for use in percentage calcs below when already dealing with rates</span>
  count_to_percentage<span class="p">(</span>variable_choices<span class="p">)</span> <span class="o">%&gt;%</span> <span class="c1"># Create percentages</span>
  base<span class="o">::</span><span class="kp">replace</span><span class="p">(</span><span class="kp">is.na</span><span class="p">(</span><span class="m">.</span><span class="p">),</span> <span class="m">0</span><span class="p">)</span> <span class="o">%&gt;%</span>  <span class="c1"># Replace any nan&#39;s with 0&#39;s </span>
  dplyr<span class="o">::</span>mutate<span class="p">(</span>u062_u065 <span class="o">=</span> u062 <span class="o">+</span> u065<span class="p">)</span> <span class="o">%&gt;%</span>  <span class="c1"># Combine married/cohabiting</span>
  dplyr<span class="o">::</span>mutate<span class="p">(</span>u063_u066 <span class="o">=</span> u063 <span class="o">+</span> u066<span class="p">)</span> <span class="o">%&gt;%</span>  <span class="c1"># Combine married/cohabiting</span>
  dplyr<span class="o">::</span>mutate<span class="p">(</span>u064_u067 <span class="o">=</span> u064 <span class="o">+</span> u067<span class="p">)</span> <span class="o">%&gt;%</span>  <span class="c1"># Combine married/cohabiting</span>
  dplyr<span class="o">::</span>mutate<span class="p">(</span>u049_u050 <span class="o">=</span> u049 <span class="o">+</span> u050<span class="p">)</span> <span class="o">%&gt;%</span>  <span class="c1"># Combine english skills</span>
  dplyr<span class="o">::</span>mutate<span class="p">(</span>u127_u129 <span class="o">=</span> u127 <span class="o">+</span> u129<span class="p">)</span> <span class="o">%&gt;%</span>  <span class="c1"># Comnine students</span>
  dplyr<span class="o">::</span>mutate<span class="p">(</span>u095_u096 <span class="o">=</span> u095 <span class="o">+</span> u096<span class="p">)</span> <span class="o">%&gt;%</span>  <span class="c1"># combine occupancy rating</span>
  dplyr<span class="o">::</span>select<span class="p">(</span><span class="o">-</span>u062<span class="o">:-</span>u067<span class="p">)</span> <span class="o">%&gt;%</span>  <span class="c1"># Remove married/cohabiting</span>
  dplyr<span class="o">::</span>select<span class="p">(</span><span class="o">-</span>u047<span class="o">:-</span>u050<span class="p">)</span> <span class="o">%&gt;%</span>  <span class="c1"># Remove english skills</span>
  dplyr<span class="o">::</span>select<span class="p">(</span><span class="o">-</span>u127<span class="p">)</span> <span class="o">%&gt;%</span>  <span class="c1"># remove students</span>
  dplyr<span class="o">::</span>select<span class="p">(</span><span class="o">-</span>u129<span class="p">)</span> <span class="o">%&gt;%</span>  <span class="c1"># remove students</span>
  dplyr<span class="o">::</span>select<span class="p">(</span><span class="o">-</span>u095<span class="o">:-</span>u099<span class="p">)</span>  <span class="c1"># remove occupancy</span>

cv_pct_data <span class="o">&lt;-</span> cv_pct_data<span class="p">[,</span> <span class="kp">order</span><span class="p">(</span><span class="kp">names</span><span class="p">(</span>cv_pct_data<span class="p">))]</span> <span class="c1"># sort colums by name # sort colums by name</span>

<span class="c1"># Make lookup maps</span>
keys <span class="o">&lt;-</span> <span class="kp">as.list</span><span class="p">(</span><span class="kt">c</span><span class="p">(</span>variable_choices<span class="o">$</span>VariableCode<span class="p">,</span> 
                  <span class="kt">c</span><span class="p">(</span><span class="s">&quot;OA&quot;</span><span class="p">,</span> 
                    <span class="s">&quot;u062_u065&quot;</span><span class="p">,</span>
                    <span class="s">&quot;u063_u066&quot;</span><span class="p">,</span> 
                    <span class="s">&quot;u064_u067&quot;</span><span class="p">,</span>
                    <span class="s">&quot;u049_u050&quot;</span><span class="p">,</span>
                    <span class="s">&quot;u127_u129&quot;</span><span class="p">,</span>
                    <span class="s">&quot;u095_u096&quot;</span>
                    <span class="p">)))</span>

values <span class="o">&lt;-</span> <span class="kp">as.list</span><span class="p">(</span><span class="kt">c</span><span class="p">(</span>variable_choices<span class="o">$</span>VariableLabel<span class="p">,</span> 
                    <span class="kt">c</span><span class="p">(</span><span class="s">&quot;Output Area&quot;</span><span class="p">,</span> <span class="s">&quot;Married or cohabiting: No children&quot;</span><span class="p">,</span> 
                      <span class="s">&quot;Married or cohabiting: Dependent children&quot;</span><span class="p">,</span> 
                      <span class="s">&quot;Married or cohabiting: No dependent children&quot;</span><span class="p">,</span>
                      <span class="s">&quot;Poor English Skils&quot;</span><span class="p">,</span>
                      <span class="s">&quot;Student&quot;</span><span class="p">,</span>
                      <span class="s">&quot;Crowded houseing&quot;</span>
                      <span class="p">)))</span>

lookup_map <span class="o">&lt;-</span> <span class="kp">list2env</span><span class="p">(</span>setNames<span class="p">(</span>values<span class="p">,</span> keys<span class="p">),</span> envir <span class="o">=</span> <span class="kp">new.env</span><span class="p">(</span>hash <span class="o">=</span> <span class="kc">TRUE</span><span class="p">))</span>
rev_lookup_map <span class="o">&lt;-</span> <span class="kp">list2env</span><span class="p">(</span>setNames<span class="p">(</span>keys<span class="p">,</span> values<span class="p">),</span> envir <span class="o">=</span> <span class="kp">new.env</span><span class="p">(</span>hash <span class="o">=</span> <span class="kc">TRUE</span><span class="p">))</span>
</pre></div>


<h4>Transformation</h4>
<p>Exploratory analysis of the data showed that there was a range of different distributions represented by my final variable choices. This was apparent through plotting histograms of each variable (I have ommited showing this step for the sake of breavity). A range of differing distributions creates a problem when using k-means as the algorithm is optimised to find spherical clusters and as such the optimal distribution for a successful k-means classification is the Gaussian distribution for input variables. In order to address this problem I performed a transformation stage. The transformation used was Inverse Hyperbolic Sine (IHS) which reduces the variance in variable distributions through increasing distance between small values and compressing distance between large values. An advantage of using IHS is that unlike some other transformations it may also be used for zero or negative values.</p>
<div class="highlight"><pre><span></span><span class="c1"># Transformation (IHS)</span>
ihs <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>input_df<span class="p">)</span> <span class="p">{</span>
  as.tibble<span class="p">(</span><span class="kp">log</span><span class="p">(</span>input_df <span class="o">+</span> <span class="kp">sqrt</span><span class="p">(</span>input_df <span class="o">^</span> <span class="m">2</span> <span class="o">+</span> <span class="m">1</span><span class="p">)))</span>
<span class="p">}</span>

cv_ihs <span class="o">&lt;-</span> ihs<span class="p">(</span>cv_pct_data<span class="p">)</span>
</pre></div>


<h4>Standardisation</h4>
<p>I also standardised the scale on which the data was measured. The method I used was the z-score which is a calculation of the number of standard deviations away from the variable mean a value is. Standardising the variables in this way is essential for effective k-means clustering as it prevents variables with larger scale ranges from being more influential and is standard practice when perfoming k-means.</p>
<div class="highlight"><pre><span></span><span class="c1"># Standardisation (Z-score)</span>
z_scores <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>input_df<span class="p">)</span> <span class="p">{</span>
  <span class="c1"># Compute the column-wise means for all observations</span>
  mean_by_col <span class="o">&lt;-</span> <span class="kp">apply</span><span class="p">(</span>input_df<span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="kp">mean</span><span class="p">)</span>
  <span class="c1"># Compute the column-wise sample sd&#39;s for *all* observations</span>
  sd_by_col <span class="o">&lt;-</span> <span class="kp">apply</span><span class="p">(</span>input_df<span class="p">,</span> <span class="m">2</span><span class="p">,</span> sd<span class="p">)</span>
  <span class="c1"># Create the z-scores via the &#39;scale&#39; function</span>
  zscores <span class="o">&lt;-</span><span class="kp">scale</span><span class="p">(</span>input_df<span class="p">,</span> center <span class="o">=</span> mean_by_col<span class="p">,</span> scale <span class="o">=</span> sd_by_col<span class="p">)</span>
  as.tibble<span class="p">(</span>zscores<span class="p">)</span>
<span class="p">}</span>

cv_ihs_z <span class="o">&lt;-</span> z_scores<span class="p">(</span>cv_ihs<span class="p">)</span>
</pre></div>


<h4>Corrolation</h4>
<p>At this point it is good idea to check for corrolation between varibles. When considering corrlations for classifications we need to consider the both reduncy in the data to to corrolation as well as how easy it will be to interperate the final clusters. For this work I used a Pearson coefficient for corrolation with a cutoff value of |&gt;0.6|.If the absolute  coefficient between two variable was above this cutoff then the variables were considered for removal.   </p>
<p>Reducing the number of variables for consumer vulnerability clustering from the initial selection was an iterative process. So I may run the following steps, view the results and return back to this point.</p>
<div class="highlight"><pre><span></span><span class="c1"># Corrolation</span>
cor_cutoff <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>variable<span class="p">,</span> cutoff<span class="p">)</span> <span class="p">{</span>
  cor_mat<span class="p">[,</span>variable<span class="p">][</span>cor_mat<span class="p">[,</span>variable<span class="p">]</span> <span class="o">&gt;=</span> cutoff <span class="o">|</span> cor_mat<span class="p">[,</span>variable<span class="p">]</span> <span class="o">&lt;=</span> <span class="o">-</span>cutoff<span class="p">]</span>
<span class="p">}</span>
cor_mat <span class="o">&lt;-</span> cor<span class="p">(</span>cv_ihs_z<span class="p">)</span>
high_cor <span class="o">&lt;-</span> purrr<span class="o">::</span>map<span class="p">(</span><span class="kp">colnames</span><span class="p">(</span>cor_mat<span class="p">),</span> cor_cutoff<span class="p">,</span> <span class="m">0.6</span><span class="p">)</span>
high_cor<span class="p">[</span>purrr<span class="o">::</span>map_lgl<span class="p">(</span>high_cor<span class="p">,</span> <span class="o">~</span><span class="kp">length</span><span class="p">(</span><span class="m">.</span><span class="p">)</span> <span class="o">&gt;</span> <span class="m">1</span><span class="p">)]</span>
</pre></div>


<h3>Principal Component Analysis</h3>
<p>After selecting the variables for clustering and performing data transformation, there were still variables that were highly correlated which were kept as they increased the interpretability of the final clusters.  There were also a reasonably large number of variables remaining despite the original cluster choices being largely cut down. To help address these issues,  I used principal component analysis (PCA) on the treated dataset before clustering.</p>
<p>PCA is a dimensionality reduction technique which uses orthogonal transformation to convert potentially correlated variables into linearly uncorrelated principal components. It is frequently used alongside k-means to reduce the dimensionally of data while retaining as much variation as possible before clustering. This helps to avoid the "curse of dimensionality" sometimes experienced when clustering data. When PCA is performed before k-means the user must decide how many principal components will be used as input for the clustering step. This was achieved by looking at the variance explained by each principal component and the cumulative variance. An elbow plot can be used to help to make this decision.</p>
<div class="highlight"><pre><span></span><span class="c1"># Perfom principle component analysis</span>
pca_obj <span class="o">&lt;-</span> stats<span class="o">::</span>princomp<span class="p">(</span>cv_ihs_z<span class="p">,</span> scores <span class="o">=</span> <span class="bp">T</span><span class="p">)</span>
</pre></div>


<p>We can look at variation each componet explains.</p>
<div class="highlight"><pre><span></span><span class="c1"># Look at variation each componet explains</span>
pca_obj<span class="o">$</span>sdev<span class="o">^</span><span class="m">2</span><span class="o">/</span><span class="kp">sum</span><span class="p">(</span>pca_obj<span class="o">$</span>sdev<span class="o">^</span><span class="m">2</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>plot<span class="p">(</span>pca_obj<span class="o">$</span>sdev<span class="o">^</span><span class="m">2</span><span class="o">/</span><span class="kp">sum</span><span class="p">(</span>pca_obj<span class="o">$</span>sdev<span class="o">^</span><span class="m">2</span><span class="p">),</span>
     <span class="c1">#main=&quot;Variance explained by priciple components&quot;,</span>
     xlab<span class="o">=</span><span class="s">&quot;Principal components&quot;</span><span class="p">,</span> 
     ylab<span class="o">=</span><span class="s">&quot;Variance&quot;</span><span class="p">,</span> 
     type <span class="o">=</span> <span class="s">&quot;o&quot;</span><span class="p">)</span>
</pre></div>


<p><img alt="center" src="https://michael-adcock.github.io/figures/consumer_vulnerability_notebook_final/unnamed-chunk-8-1.png" /></p>
<p>We can also look at the cumulative variation.</p>
<div class="highlight"><pre><span></span><span class="c1"># Look at cumulative variation</span>
<span class="kp">cumsum</span><span class="p">(</span>pca_obj<span class="o">$</span>sdev<span class="o">^</span><span class="m">2</span><span class="o">/</span><span class="kp">sum</span><span class="p">(</span>pca_obj<span class="o">$</span>sdev<span class="o">^</span><span class="m">2</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>plot<span class="p">(</span><span class="kp">cumsum</span><span class="p">(</span>pca_obj<span class="o">$</span>sdev<span class="o">^</span><span class="m">2</span><span class="o">/</span><span class="kp">sum</span><span class="p">(</span>pca_obj<span class="o">$</span>sdev<span class="o">^</span><span class="m">2</span><span class="p">)),</span> 
     <span class="c1">#main=&quot;Cumulative variance explained by priciple components&quot;,</span>
     xlab<span class="o">=</span><span class="s">&quot;Principal Components&quot;</span><span class="p">,</span> 
     ylab<span class="o">=</span><span class="s">&quot;Cumulative Variance&quot;</span><span class="p">,</span>
     type <span class="o">=</span> <span class="s">&quot;o&quot;</span><span class="p">)</span>
</pre></div>


<p><img alt="center" src="https://michael-adcock.github.io/figures/consumer_vulnerability_notebook_final/unnamed-chunk-9-1.png" /></p>
<p>When IHS transformation is used the first 10 priciple components explain ??% of the variation.</p>
<div class="highlight"><pre><span></span>num_pc <span class="o">&lt;-</span> <span class="m">10</span>
</pre></div>


<p>In order to ensure that the firt 10 suffiently represent the our chosen varibales, I examined the principal component loadings. In order explore this the loadings for the first 10 principal components were summed.</p>
<div class="highlight"><pre><span></span><span class="c1">#Loadings</span>
load <span class="o">&lt;-</span> <span class="kp">with</span><span class="p">(</span>pca_obj<span class="p">,</span> <span class="kp">unclass</span><span class="p">(</span>loadings<span class="p">))</span>
loadings <span class="o">&lt;-</span> <span class="kp">sweep</span><span class="p">(</span><span class="kp">abs</span><span class="p">(</span><span class="kp">load</span><span class="p">),</span> <span class="m">2</span><span class="p">,</span> <span class="kp">colSums</span><span class="p">(</span><span class="kp">abs</span><span class="p">(</span><span class="kp">load</span><span class="p">)),</span> <span class="s">&quot;/&quot;</span><span class="p">)</span>

par<span class="p">(</span>mar<span class="o">=</span><span class="kt">c</span><span class="p">(</span><span class="m">15</span><span class="p">,</span> <span class="m">5</span><span class="p">,</span><span class="m">1</span><span class="p">,</span><span class="m">1</span><span class="p">))</span>
barplot<span class="p">(</span><span class="kp">rowSums</span><span class="p">(</span><span class="kp">abs</span><span class="p">(</span><span class="kp">load</span><span class="p">[,</span><span class="m">1</span><span class="o">:</span>num_pc<span class="p">])),</span> las<span class="o">=</span><span class="m">2</span><span class="p">,</span> cex.names <span class="o">=</span> <span class="m">0.6</span><span class="p">,</span> <span class="c1">#main = &quot;Sum of Loadings for Principal components 1 to 10&quot;, </span>
        ylab <span class="o">=</span> <span class="s">&quot;Sum of Principal Componet Loadings&quot;</span><span class="p">,</span> border <span class="o">=</span> <span class="s">&quot;blue&quot;</span><span class="p">)</span>
</pre></div>


<p><img alt="center" src="https://michael-adcock.github.io/figures/consumer_vulnerability_notebook_final/unnamed-chunk-11-1.png" /></p>
<h3>K-means Clustering</h3>
<p>K-means is an iterative algorithm which finds clusters in data based on the centroids of clusters. Centroid position is random for initial step and subsequently updated at each iteration based on the previous cluster assignments . The initial cluster centroids are randomly selected and the number of clusters (k) must be chosen by the user. </p>
<p>I performed k-means using the first 10 principal components generated from the PCA step as input. The optimum number of clusters was first determined by running k-means at each value of k from 2 to 25. Scree plots and smallest cluster plots from the results were then created. A scree or elbow plot shows the within cluster sum of squares for each cluster number when running k-means. The aim of such a plot is to show where the elbow is, suggesting an appropriate number of clusters, as there are diminishing returns after this point. A smallest cluster plot shows the size of the smallest cluster size when k-means is run for each number of clusters. This allows us to see at which point we start the get very small clusters, which is undesirable. </p>
<p>As the initialisation of centroids for k-means clustering was random, clustering was repeated for each value of k 25 times with different random centroid placement. From these 25 runs the result with the lowest total within sum of squares were chosen for the result.  Once an appropriate number of clusters was chosen, I ran k-means again on the dataset 100 times at the selected number for k and the solution with the lowest within sum of squares was selected. </p>
<div class="highlight"><pre><span></span><span class="c1"># Wrapper for kmeans</span>
k_means <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>k<span class="p">,</span> input_data<span class="p">,</span> n_start_input<span class="p">)</span> <span class="p">{</span>
  <span class="kp">set.seed</span><span class="p">(</span><span class="m">2494</span><span class="p">)</span>
  kmeans<span class="p">(</span>input_data<span class="p">,</span> 
         k<span class="p">,</span> iter.max <span class="o">=</span> <span class="m">1000</span><span class="p">,</span> 
         nstart <span class="o">=</span> n_start_input<span class="p">,</span> 
         algorithm <span class="o">=</span> <span class="s">&quot;Lloyd&quot;</span><span class="p">)}</span>

k_comparison <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>input_data<span class="p">,</span> iter<span class="o">=</span><span class="m">1</span><span class="o">:</span><span class="m">15</span><span class="p">,</span> n_start<span class="o">=</span><span class="m">25</span><span class="p">)</span> <span class="p">{</span>
  <span class="c1"># Set up parrallel processing for windows</span>
  cl <span class="o">&lt;-</span> makeCluster<span class="p">(</span>detectCores<span class="p">()</span> <span class="o">-</span> <span class="m">1</span><span class="p">)</span>
  clusterExport<span class="p">(</span>cl<span class="o">=</span>cl<span class="p">,</span>
                varlist<span class="o">=</span><span class="kt">c</span><span class="p">(</span><span class="s">&quot;iter&quot;</span><span class="p">,</span> <span class="s">&quot;k_means&quot;</span><span class="p">,</span> <span class="s">&quot;input_data&quot;</span><span class="p">,</span> <span class="s">&quot;n_start&quot;</span><span class="p">),</span>
                envir<span class="o">=</span><span class="kp">environment</span><span class="p">())</span>
  <span class="kp">system.time</span><span class="p">(</span>
    k_choice_clust_list <span class="o">&lt;-</span> parLapply<span class="p">(</span>cl<span class="p">,</span> iter<span class="p">,</span> k_means<span class="p">,</span> input_data<span class="p">,</span> n_start<span class="p">)</span>
  <span class="p">)</span>
  stopCluster<span class="p">(</span>cl<span class="p">)</span>
  <span class="kr">return</span><span class="p">(</span>k_choice_clust_list<span class="p">)</span>
<span class="p">}</span>

<span class="c1"># Scree plot</span>
scree <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>k_choice_clust_list<span class="p">,</span> iter<span class="p">,</span> title<span class="p">)</span> <span class="p">{</span>
  wss <span class="o">&lt;-</span> purrr<span class="o">::</span>map<span class="p">(</span>k_choice_clust_list<span class="p">,</span> <span class="o">~</span><span class="m">.</span><span class="o">$</span>tot.withinss<span class="p">)</span>
  plot<span class="p">(</span>iter<span class="p">,</span> wss<span class="p">,</span> 
       type <span class="o">=</span> <span class="s">&quot;o&quot;</span><span class="p">,</span> 
       main <span class="o">=</span> <span class="kp">paste</span><span class="p">(</span><span class="s">&quot;Cluster Scree Plot: &quot;</span><span class="p">,</span> title<span class="p">),</span>
       xlab <span class="o">=</span> <span class="s">&quot;Number of Clusters&quot;</span><span class="p">,</span> 
       ylab <span class="o">=</span> <span class="s">&quot;Within Cluster Sum of Squares&quot;</span><span class="p">)</span>

  min_clus <span class="o">&lt;-</span>purrr<span class="o">::</span>map<span class="p">(</span>k_choice_clust_list<span class="p">,</span> <span class="o">~</span><span class="kp">min</span><span class="p">(</span><span class="m">.</span><span class="o">$</span>size<span class="p">))</span>
  plot<span class="p">(</span>iter<span class="p">,</span> min_clus<span class="p">,</span> 
       type <span class="o">=</span> <span class="s">&quot;h&quot;</span><span class="p">,</span> 
       main <span class="o">=</span> <span class="kp">paste</span><span class="p">(</span><span class="s">&quot;Smallest Cluster Plot:&quot;</span><span class="p">,</span> title<span class="p">),</span>
       xlab <span class="o">=</span> <span class="s">&quot;Number of Clusters&quot;</span><span class="p">,</span> 
       ylab <span class="o">=</span> <span class="s">&quot;Smallest Cluster Size&quot;</span><span class="p">)</span>
<span class="p">}</span>

iter <span class="o">=</span> <span class="m">1</span><span class="o">:</span><span class="m">25</span>
k_choice_clust_list <span class="o">&lt;-</span> k_comparison<span class="p">(</span>pca_obj<span class="o">$</span>scores<span class="p">[,</span> <span class="m">1</span><span class="o">:</span>num_pc<span class="p">],</span> iter<span class="p">,</span> <span class="m">25</span><span class="p">)</span>
scree<span class="p">(</span>k_choice_clust_list<span class="p">,</span> iter<span class="p">,</span> <span class="s">&quot;All data&quot;</span><span class="p">)</span>
</pre></div>


<p><img alt="center" src="https://michael-adcock.github.io/figures/consumer_vulnerability_notebook_final/unnamed-chunk-12-1.png" />
<img alt="center" src="https://michael-adcock.github.io/figures/consumer_vulnerability_notebook_final/unnamed-chunk-12-2.png" /></p>
<p>Small clusters start after a k of 6. The elbow plot does not have avery clear cutoff by does not dissagree with k of 6.</p>
<div class="highlight"><pre><span></span>num_clusters <span class="o">&lt;-</span> <span class="m">6</span>

nstart<span class="o">=</span><span class="m">100</span>
<span class="kp">set.seed</span><span class="p">(</span><span class="m">2494</span><span class="p">)</span>
<span class="kp">system.time</span><span class="p">(</span>
  k_means_obj <span class="o">&lt;-</span> k_means<span class="p">(</span>num_clusters<span class="p">,</span> pca_obj<span class="o">$</span>scores<span class="p">[,</span> <span class="m">1</span><span class="o">:</span>num_pc<span class="p">],</span> nstart<span class="p">)</span>
<span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>##    user  system elapsed 
## 856.426   0.000 860.510
</pre></div>


<div class="highlight"><pre><span></span>cluster_names <span class="o">&lt;-</span> <span class="kt">c</span><span class="p">(</span><span class="s">&quot;Students and Young Professionals&quot;</span><span class="p">,</span> <span class="s">&quot;On a Budget&quot;</span><span class="p">,</span> <span class="s">&quot;Prosperous Professionals&quot;</span><span class="p">,</span>
                   <span class="s">&quot;Well Established&quot;</span><span class="p">,</span> <span class="s">&quot;Vulnerable Communities&quot;</span><span class="p">,</span> <span class="s">&quot;Vulnerable Pensioners&quot;</span><span class="p">)</span>
</pre></div>


<p>To aid interpretation we can view how each cluster realtes to each variable (rather than principal components).</p>
<div class="highlight"><pre><span></span><span class="c1"># Compute a data frame (one row per cluster) containing the means of each variable in that cluster</span>
cluster_zscores <span class="o">&lt;-</span> plyr<span class="o">::</span>ddply<span class="p">(</span>cv_ihs_z<span class="p">,</span> <span class="m">.</span><span class="p">(</span>k_means_obj<span class="o">$</span>cluster<span class="p">),</span> numcolwise<span class="p">(</span><span class="kp">mean</span><span class="p">))[,</span> <span class="m">-1</span><span class="p">]</span>
<span class="kp">row.names</span><span class="p">(</span>cluster_zscores<span class="p">)</span> <span class="o">&lt;-</span> cluster_names
</pre></div>


<h3>Radial Plots</h3>
<h4>Groups</h4>
<p>Radial plots were produced for each cluster showing the z-scores for each variable. These show the strength of each characteristic in each cluster.</p>
<div class="highlight"><pre><span></span><span class="c1"># produce radial plots</span>
radial_plot <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>data<span class="p">,</span> main_title<span class="p">,</span> min_max<span class="o">=</span><span class="kt">c</span><span class="p">(</span><span class="m">-2</span><span class="p">,</span><span class="m">3</span><span class="p">),</span> legend_names<span class="p">)</span> <span class="p">{</span>
  <span class="kp">print</span><span class="p">(</span>main_title<span class="p">)</span>
  par<span class="p">(</span>cex.axis<span class="o">=</span><span class="m">.7</span><span class="p">)</span>
  par<span class="p">(</span>cex.lab<span class="o">=</span><span class="m">.6</span><span class="p">)</span>
  colours <span class="o">&lt;-</span> brewer.pal<span class="p">(</span><span class="m">8</span><span class="p">,</span><span class="s">&#39;Set1&#39;</span><span class="p">)</span>
  p <span class="o">&lt;-</span> radial.plot<span class="p">(</span>data<span class="p">,</span> labels<span class="o">=</span><span class="kp">names</span><span class="p">(</span>data<span class="p">),</span>
              start<span class="o">=</span><span class="m">5</span><span class="p">,</span>
              clockwise<span class="o">=</span><span class="kc">TRUE</span><span class="p">,</span>
              rp.type<span class="o">=</span><span class="s">&quot;p&quot;</span><span class="p">,</span>
              line.col<span class="o">=</span>colours<span class="p">,</span>
              lwd<span class="o">=</span><span class="m">1</span><span class="p">,</span>
              point.col<span class="o">=</span><span class="s">&quot;black&quot;</span><span class="p">,</span> 
              label.prop<span class="o">=</span><span class="m">0.91</span><span class="p">,</span>
              radlab<span class="o">=</span><span class="kc">TRUE</span><span class="p">,</span>
              radial.lim <span class="o">=</span> min_max<span class="p">,</span>
              grid.col <span class="o">=</span> <span class="s">&quot;#E0E0E0&quot;</span><span class="p">,</span>
              col.sub <span class="o">=</span> <span class="s">&quot;green&quot;</span><span class="p">,</span>
              mar<span class="o">=</span><span class="kt">c</span><span class="p">(</span><span class="m">2</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="m">5</span><span class="p">,</span> <span class="m">0</span><span class="p">),</span>
              show.radial.grid<span class="o">=</span><span class="kc">TRUE</span>

  <span class="p">)</span>
  title<span class="p">(</span><span class="kp">paste</span><span class="p">(</span>main_title<span class="p">,</span> <span class="s">&quot;\n\n&quot;</span><span class="p">))</span>
  par<span class="p">(</span>xpd<span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span> 
  legend<span class="p">(</span><span class="m">9.5</span><span class="p">,</span><span class="m">-10</span><span class="p">,</span>legend_names<span class="p">,</span>lty<span class="o">=</span><span class="m">1</span><span class="p">,</span>lwd<span class="o">=</span><span class="m">1</span><span class="p">,</span>col<span class="o">=</span>colours<span class="p">,</span> cex<span class="o">=</span><span class="m">0.7</span><span class="p">,</span> bty <span class="o">=</span> <span class="s">&quot;n&quot;</span><span class="p">)</span>
  par<span class="p">(</span>xpd<span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span> 
<span class="p">}</span>


<span class="c1"># groups to global</span>
purrr<span class="o">::</span>map<span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="kp">nrow</span><span class="p">(</span>cluster_zscores<span class="p">),</span>
    <span class="kr">function</span><span class="p">(</span>cluster_z_diff<span class="p">,</span> x<span class="p">)</span> radial_plot<span class="p">(</span>cluster_zscores<span class="p">[</span>x<span class="p">,],</span> cluster_names<span class="p">[</span>x<span class="p">],</span> min_max<span class="o">=</span><span class="kt">c</span><span class="p">(</span><span class="m">-3</span><span class="p">,</span><span class="m">3</span><span class="p">),</span> <span class="s">&quot;Group to global \n mean z-score\n&quot;</span><span class="p">))</span>
</pre></div>


<p><img alt="center" src="https://michael-adcock.github.io/figures/prosperous_professionals.png" />
<img alt="center" src="https://michael-adcock.github.io/figures/Well_Established.png" />
<img alt="center" src="https://michael-adcock.github.io/figures/students_and_young_professionals.png" />
<img alt="center" src="https://michael-adcock.github.io/figures/on_a_budget.png" />
<img alt="center" src="https://michael-adcock.github.io/figures/Vulnerable_Communities.png" />
<img alt="center" src="https://michael-adcock.github.io/figures/Vulnerable_Pensioners.png" /></p>
<h3>Group Heatmap</h3>
<p>A heatmap was produced showing the z-scores for each variable in each cluster in a single figure. The dendogram produced with the heatmap by hierarchical clustering of the six clusters grouped "Vulnerable Pensioners" and "Vulnerable Communities" with similarities in education, vehicle ownership, and employment and housing variables. The "Well Established", "Prosperous Professional" and "On a Budget" clusters were also grouped together with similarities in housing, vehicle and living arrangement variables. The "Students and Young Professionals" were not very strongly grouped with any other clusters but closer to "Well Established", "Prosperous Professional" and "On a Budget" clusters than "Vulnerable Pensioners" and "Vulnerable Communities".</p>
<div class="highlight"><pre><span></span>create_heatmap <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>mean_cluster_zscores<span class="p">,</span> num_clusters<span class="p">,</span> margins<span class="p">)</span> <span class="p">{</span>
  <span class="c1"># Create heatmap</span>
  colours <span class="o">&lt;-</span> <span class="kp">rev</span><span class="p">(</span>brewer.pal<span class="p">(</span><span class="m">10</span><span class="p">,</span><span class="s">&#39;BrBG&#39;</span><span class="p">))</span> <span class="c1"># reverse colours</span>
  break_nums <span class="o">&lt;-</span> <span class="kt">c</span><span class="p">(</span><span class="m">-1e10</span><span class="p">,</span><span class="m">-2</span><span class="p">,</span><span class="m">-1.5</span><span class="p">,</span><span class="m">-1</span><span class="p">,</span><span class="m">-0.5</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">0.5</span><span class="p">,</span><span class="m">1</span><span class="p">,</span><span class="m">1.5</span><span class="p">,</span><span class="m">2</span><span class="p">,</span><span class="m">+1e10</span><span class="p">)</span> 
  legend_names <span class="o">&lt;-</span> <span class="kt">c</span><span class="p">(</span><span class="s">&quot;&lt;-2&quot;</span><span class="p">,</span><span class="s">&quot;-2 to -1.5&quot;</span><span class="p">,</span><span class="s">&quot;-1.5 to -1&quot;</span><span class="p">,</span><span class="s">&quot;-1 to -0.5&quot;</span><span class="p">,</span><span class="s">&quot;-0.5 to 0&quot;</span><span class="p">,</span><span class="s">&quot;0 to 0.5&quot;</span><span class="p">,</span><span class="s">&quot;1 to 2&quot;</span><span class="p">,</span><span class="s">&quot;1.5 to 2&quot;</span><span class="p">,</span><span class="s">&quot;2 to 2.5&quot;</span><span class="p">,</span><span class="s">&quot;&gt;2&quot;</span><span class="p">)</span>
  heatmap<span class="p">(</span><span class="kp">t</span><span class="p">(</span>mean_cluster_zscores<span class="p">),</span>
          scale <span class="o">=</span> <span class="s">&#39;none&#39;</span><span class="p">,</span>
          col <span class="o">=</span> colours<span class="p">,</span>

          breaks <span class="o">=</span> break_nums<span class="p">,</span> <span class="c1">#3</span>
          cexRow <span class="o">=</span> <span class="m">.5</span><span class="p">,</span>
          cexCol <span class="o">=</span> <span class="m">.8</span><span class="p">,</span>
          mar<span class="o">=</span>margins<span class="p">,</span>
          add.expr<span class="o">=</span> abline<span class="p">(</span>h <span class="o">=</span> <span class="p">(</span><span class="m">0</span><span class="o">:</span><span class="kp">length</span><span class="p">(</span>mean_cluster_zscores<span class="p">))</span> <span class="o">+</span> <span class="m">0.5</span><span class="p">,</span> v <span class="o">=</span> <span class="p">(</span><span class="m">0</span><span class="o">:</span>num_clusters<span class="p">)</span> <span class="o">+</span> <span class="m">0.5</span><span class="p">,</span> col <span class="o">=</span> <span class="s">&#39;white&#39;</span><span class="p">))</span>
  par<span class="p">(</span>xpd<span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span> 
  legend<span class="p">(</span>title<span class="o">=</span><span class="kp">expression</span><span class="p">(</span>bold<span class="p">(</span><span class="s">&quot;z-score\n\n&quot;</span><span class="p">)),</span> <span class="m">0.22</span><span class="p">,</span> <span class="m">-0.07</span><span class="p">,</span> legend_names<span class="p">,</span> title.adj<span class="o">=</span><span class="m">0.48</span><span class="p">,</span> lty <span class="o">=</span> <span class="m">1</span><span class="p">,</span> lwd <span class="o">=</span> <span class="m">20</span><span class="p">,</span> col<span class="o">=</span>colours<span class="p">,</span> cex<span class="o">=</span><span class="m">0.7</span><span class="p">,</span> bty <span class="o">=</span> <span class="s">&quot;n&quot;</span><span class="p">,</span> horiz<span class="o">=</span><span class="kc">TRUE</span><span class="p">,</span> adj<span class="o">=</span><span class="kt">c</span><span class="p">(</span><span class="m">0.5</span><span class="p">,</span> <span class="m">-2</span><span class="p">),</span> x.intersp<span class="o">=</span><span class="m">-1</span><span class="p">)</span>
  par<span class="p">(</span>xpd<span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span> 
 <span class="p">}</span>


<span class="c1"># groups heatmap </span>
sub_cluster_zscores <span class="o">&lt;-</span> map2<span class="p">(</span>sub_cluster_zscores<span class="p">,</span> cluster_names<span class="p">,</span> rename_rows<span class="p">)</span>
<span class="c1">#pdf(&quot;figures\\group_heatmap.pdf&quot;, width = 250, height=350)</span>
create_heatmap<span class="p">(</span>cluster_zscores<span class="p">,</span> <span class="m">6</span><span class="p">,</span> <span class="kt">c</span><span class="p">(</span><span class="m">16</span><span class="p">,</span><span class="m">7</span><span class="p">))</span>
</pre></div>


<p><img alt="center" src="https://michael-adcock.github.io/figures/group_heatmap.png" /></p>
<h3>Sub-clusters</h3>
<p>In order to explore the clusters further, k-means clustering was applied to each cluster separately to create sub-clusters. I acheceved this using the methods as previously but by fist subsetting the data for each cluster.</p>
<div class="highlight"><pre><span></span>run_kmeans_comp_subgroup <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>subgroup<span class="p">,</span> data<span class="p">,</span> num_pc<span class="p">)</span> <span class="p">{</span>
  data <span class="o">%&gt;%</span>
    filter<span class="p">(</span>cluster<span class="o">==</span>subgroup<span class="p">)</span> <span class="o">%&gt;%</span> 
    select<span class="p">(</span><span class="m">1</span><span class="o">:</span>num_pc<span class="p">)</span> <span class="o">%&gt;%</span> 
    k_comparison<span class="p">(</span>iter<span class="p">)</span> <span class="o">%&gt;%</span> 
    scree<span class="p">(</span>iter<span class="p">,</span> <span class="kp">paste</span><span class="p">(</span><span class="s">&quot;subgroup &quot;</span><span class="p">,</span> subgroup<span class="p">))</span>
<span class="p">}</span>

run_kmeans_subgroup <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>subgroup<span class="p">,</span> k<span class="p">,</span> nstart<span class="p">)</span> <span class="p">{</span>
  subgroup <span class="o">&lt;-</span> subgroup<span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="kp">ncol</span><span class="p">(</span>subgroup<span class="p">)</span><span class="m">-1</span><span class="p">]</span>
  k_means<span class="p">(</span>k<span class="p">,</span> subgroup<span class="p">,</span> nstart<span class="p">)</span>
<span class="p">}</span>

rename_rows <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>df<span class="p">,</span> name<span class="p">)</span> <span class="p">{</span>
  <span class="kp">row.names</span><span class="p">(</span>df<span class="p">)</span> <span class="o">&lt;-</span> map<span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="kp">nrow</span><span class="p">(</span>df<span class="p">),</span> <span class="kr">function</span><span class="p">(</span><span class="kp">row</span><span class="p">,</span> name<span class="p">)</span> <span class="kp">paste</span><span class="p">(</span>name<span class="p">,</span> <span class="s">&quot;: Subgroup &quot;</span><span class="p">,</span> <span class="kp">row</span><span class="p">,</span> sep<span class="o">=</span><span class="s">&quot;&quot;</span><span class="p">),</span> name<span class="p">)</span>
  <span class="kr">return</span><span class="p">(</span>df<span class="p">)</span>
<span class="p">}</span>

cluster_tibble <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>subgroup<span class="p">,</span> kmeans_obj<span class="p">,</span> cluster_name<span class="p">)</span> <span class="p">{</span>
  tib <span class="o">&lt;-</span> tibble<span class="p">(</span>subgroup<span class="o">$</span>OA<span class="p">,</span> kmeans_obj<span class="o">$</span>cluster<span class="p">)</span>
  <span class="kp">names</span><span class="p">(</span>tib<span class="p">)</span> <span class="o">&lt;-</span> <span class="kt">c</span><span class="p">(</span><span class="s">&quot;OA_SA&quot;</span><span class="p">,</span> cluster_name<span class="p">)</span> 
  <span class="kr">return</span><span class="p">(</span>tib<span class="p">)</span>
<span class="p">}</span>

cluster_selection <span class="o">&lt;-</span> tibble<span class="o">::</span>as_tibble<span class="p">(</span><span class="kp">cbind</span><span class="p">(</span>pca_obj<span class="o">$</span>scores<span class="p">[,</span> <span class="m">1</span><span class="o">:</span>num_pc<span class="p">],</span> cluster<span class="o">=</span>k_means_obj<span class="o">$</span>cluster<span class="p">,</span> OA<span class="o">=</span>oa_data<span class="o">$</span>OA<span class="p">))</span>
map<span class="p">(</span><span class="m">1</span><span class="o">:</span>num_clusters<span class="p">,</span> run_kmeans_comp_subgroup<span class="p">,</span> cluster_selection<span class="p">,</span> num_pc<span class="p">)</span>
</pre></div>


<h3>Sub-group Heatmap</h3>
<p>To further explore the character of the clusters, k-means was run on each cluster to produce sub-clusters. The number of sub-clusters chosen for each cluster were 5 sub-clusters for "Prosperous Professionals", 4 for "Well-Established", 4 for "Students ans Young Professionals", 5 for "On a Budget", 8 for "Vulnerable Communities" and 5 for "Vulnerable Pensioners". These sub-clusters were then displayed using a heatmap  complete with dendogram showing the clustering of sub-clusters using hierarchical clustering with Euclidean distance. The hierarchical clustering revealed that many of the sub groups from the same parent group clustered together as in the case of  the "Students and Young Professionals" sub-groups which all clusters together. However some subgroups from different parents groups clustered more closely with each other than with other subgroups from their own parents group. An example of this is "Vulnerable Pensioners: Sub-group 5", "Well Established: Subgroup 3" and "On a Budget: Subgroup 3" which clustered together with similarities in age structure and population employment domains. </p>
<div class="highlight"><pre><span></span><span class="c1"># subgroups heatmap</span>
create_heatmap<span class="p">(</span><span class="kp">rbind</span><span class="p">(</span>sub_cluster_zscores<span class="p">[[</span><span class="m">1</span><span class="p">]],</span> sub_cluster_zscores<span class="p">[[</span><span class="m">2</span><span class="p">]],</span> sub_cluster_zscores<span class="p">[[</span><span class="m">3</span><span class="p">]],</span> sub_cluster_zscores<span class="p">[[</span><span class="m">4</span><span class="p">]],</span> sub_cluster_zscores<span class="p">[[</span><span class="m">5</span><span class="p">]],</span> sub_cluster_zscores<span class="p">[[</span><span class="m">6</span><span class="p">]])</span>
               <span class="p">,</span> <span class="m">31</span><span class="p">,</span> <span class="kt">c</span><span class="p">(</span><span class="m">20</span><span class="p">,</span><span class="m">7</span><span class="p">))</span>
</pre></div>


<p><img alt="center" src="https://michael-adcock.github.io/figures/heatmap.png" /></p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="https://michael-adcock.github.io/tag/r.html">R</a>
      <a href="https://michael-adcock.github.io/tag/k-means.html">k-means</a>
      <a href="https://michael-adcock.github.io/tag/principal-component-analysis.html">principal component analysis</a>
      <a href="https://michael-adcock.github.io/tag/pca.html">PCA</a>
      <a href="https://michael-adcock.github.io/tag/census.html">census</a>
      <a href="https://michael-adcock.github.io/tag/consumer-vulnerability.html">consumer vulnerability</a>
    </p>
  </div>




</article>

    <footer>
<p>&copy; Michael Adcock </p>
<p>    Powered by <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a>
</p>    </footer>
  </main>





<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Michael Adcock ",
  "url" : "https://michael-adcock.github.io",
  "image": "",
  "description": ""
}
</script>
</body>
</html>